{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def fix_json(input_file, output_file):\n",
    "    # Read the JSON file as text\n",
    "    with open(input_file, 'r') as f:\n",
    "        json_text = f.read()\n",
    "\n",
    "    # Split the JSON text into individual objects\n",
    "    json_objects = json_text.split('}\\n')\n",
    "\n",
    "    # Initialize an empty list to store parsed JSON objects\n",
    "    data = []\n",
    "\n",
    "    # Process each JSON object\n",
    "    for obj in json_objects:\n",
    "        if obj.strip():  # Check if the object is not empty\n",
    "            try:\n",
    "                # Fix single quotes and append to the list\n",
    "                data.append(json.loads(obj.replace(\"'\", \"\\\"\") + '}'))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"Error decoding JSON object:\", e)\n",
    "                print(\"JSON object causing the error:\", obj)\n",
    "\n",
    "    # Convert the list of JSON objects into a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Write the DataFrame back to a JSON file with corrected formatting\n",
    "    df.to_json(output_file, orient='records')\n",
    "\n",
    "    print(\"JSON file has been fixed and saved to\", output_file)\n",
    "\n",
    "# Provide the path to your input and output JSON files\n",
    "input_file = \"meta_Automotive.json\"\n",
    "output_file = \"fix_meta_Automotive.json\"\n",
    "\n",
    "fix_json(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to format JSON objects individually\n",
    "def format_json_objects(input_file, output_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "# Example usage:\n",
    "input_file = 'fix_meta_Automotive.json'\n",
    "output_file = 'formatted_meta_Automotive.json'\n",
    "\n",
    "format_json_objects(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the ratings data from the CSV file\n",
    "ratings_path = 'ratings_Automotive.csv'\n",
    "ratings_data = pd.read_csv(ratings_path)\n",
    "# Renaming the columns for clarity\n",
    "ratings_data.columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "ratings_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_large_json(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Use the function to load your JSON metadata\n",
    "metadata_df = load_large_json('formatted_meta_Automotive.json')\n",
    "\n",
    "full_data = ratings_data.merge(metadata_df, how='left', left_on='item_id', right_on='asin')\n",
    "\n",
    "# Drop the 'asin' column if you decide it's redundant\n",
    "full_data.drop('asin', axis=1, inplace=True)\n",
    "# Display the first few rows of the dataset and its basic information\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = full_data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(full_data)) * 100\n",
    "missing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n",
    "print(\"\\nMissing Values:\\n\", missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(full_data.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(full_data.isnull().sum())\n",
    "\n",
    "# Visualize distributions of numerical variables\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histogram for price\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(full_data['price'], bins=50, kde=True)\n",
    "plt.title('Price Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Histogram for ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(full_data['rating'], bins=5, kde=True)\n",
    "plt.title('Rating Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Box plot for ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='rating', data=full_data)\n",
    "plt.title('Rating Box Plot')\n",
    "plt.show()\n",
    "\n",
    "# Select only the numeric columns for correlation matrix calculation\n",
    "numeric_cols = full_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = full_data[numeric_cols].corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the data\n",
    "# Fill missing values\n",
    "full_data['price'].fillna(full_data['price'].mean(), inplace=True)\n",
    "full_data['brand'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Fill missing values in description, title, imUrl, related\n",
    "full_data['description'].fillna('No description', inplace=True)\n",
    "full_data['title'].fillna('No title', inplace=True)\n",
    "full_data['imUrl'].fillna('No image', inplace=True)\n",
    "full_data['related'].fillna('No related items', inplace=True)\n",
    "\n",
    "# Drop the salesRank column due to high percentage of missing values\n",
    "full_data.drop(columns=['salesRank'], inplace=True)\n",
    "\n",
    "# Convert list of categories to a single string\n",
    "def convert_categories(cat):\n",
    "    if isinstance(cat, list):\n",
    "        # Flatten the list in case of nested lists\n",
    "        flat_list = [item for sublist in cat for item in sublist] if any(isinstance(i, list) for i in cat) else cat\n",
    "        return ' '.join(flat_list)\n",
    "    elif pd.isna(cat):\n",
    "        return ''\n",
    "    else:\n",
    "        return cat\n",
    "\n",
    "full_data['categories'] = full_data['categories'].apply(convert_categories)\n",
    "\n",
    "# Encode categorical variables\n",
    "full_data['categories'] = full_data['categories'].astype('category').cat.codes\n",
    "full_data['brand'] = full_data['brand'].astype('category').cat.codes\n",
    "\n",
    "# Save pre-processed data to a new CSV file for future use\n",
    "full_data.to_csv('preprocessed_combined_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = full_data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(full_data)) * 100\n",
    "missing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n",
    "print(\"\\nMissing Values:\\n\", missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load pre-processed data\n",
    "combined_df = pd.read_csv('preprocessed_combined_data.csv')\n",
    "\n",
    "# Fill missing text with a placeholder\n",
    "combined_df['title'].fillna('No Title', inplace=True)\n",
    "combined_df['description'].fillna('No Description', inplace=True)\n",
    "\n",
    "# Select a subset of the data for testing\n",
    "combined_df = combined_df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Apply TF-IDF Vectorization separately\n",
    "tfidf_vectorizer_title = TfidfVectorizer(stop_words='english', max_features=2500)  # Half the features for title\n",
    "tfidf_vectorizer_description = TfidfVectorizer(stop_words='english', max_features=2500)  # Half for description\n",
    "\n",
    "title_tfidf_matrix = tfidf_vectorizer_title.fit_transform(combined_df['title'])\n",
    "description_tfidf_matrix = tfidf_vectorizer_description.fit_transform(combined_df['description'])\n",
    "\n",
    "# Dimensionality reduction (optional)\n",
    "svd_title = TruncatedSVD(n_components=50)  # Reduce dimensions for title\n",
    "svd_description = TruncatedSVD(n_components=50)  # Reduce for description\n",
    "\n",
    "title_tfidf_reduced = svd_title.fit_transform(title_tfidf_matrix)\n",
    "description_tfidf_reduced = svd_description.fit_transform(description_tfidf_matrix)\n",
    "\n",
    "# Combine TF-IDF with Rating Data\n",
    "title_df = pd.DataFrame(title_tfidf_reduced, index=combined_df.index)\n",
    "description_df = pd.DataFrame(description_tfidf_reduced, index=combined_df.index)\n",
    "full_feature_df = pd.concat([title_df, description_df], axis=1)\n",
    "\n",
    "full_data = pd.concat([combined_df[['user_id', 'item_id', 'rating']], full_feature_df], axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(full_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Example model: Random Forest\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(train_data.drop(['user_id', 'item_id', 'rating'], axis=1), train_data['rating'])\n",
    "\n",
    "# Predict ratings\n",
    "predicted_ratings = model.predict(test_data.drop(['user_id', 'item_id', 'rating'], axis=1))\n",
    "\n",
    "# Evaluation\n",
    "rmse = mean_squared_error(test_data['rating'], predicted_ratings, squared=False)\n",
    "mae = mean_absolute_error(test_data['rating'], predicted_ratings)\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Define a function to load data in chunks\n",
    "def load_data_in_chunks(file_path, chunk_size=10000):\n",
    "    chunk_list = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        chunk_list.append(chunk)\n",
    "    return pd.concat(chunk_list, axis=0)\n",
    "\n",
    "# Load pre-processed data in chunks\n",
    "file_path = 'preprocessed_combined_data.csv'\n",
    "combined_df = load_data_in_chunks(file_path, chunk_size=10000)\n",
    "\n",
    "# Shuffle the entire dataset\n",
    "combined_df_subset = combined_df.sample(frac=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a pivot table for items and features\n",
    "item_features = combined_df_subset.pivot_table(index='item_id', columns='user_id', values='rating').fillna(0)\n",
    "\n",
    "# Convert the pivot table to a sparse matrix\n",
    "item_features_sparse = csr_matrix(item_features.values)\n",
    "\n",
    "# Calculate cosine similarity between items\n",
    "item_similarity = cosine_similarity(item_features_sparse)\n",
    "\n",
    "# Convert the similarity matrix to a DataFrame for easier handling\n",
    "item_similarity_df = pd.DataFrame(item_similarity, index=item_features.index, columns=item_features.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Function to predict ratings using item similarity\n",
    "def predict_ratings_batch(user_id, item_id):\n",
    "    if item_id in item_features.index:\n",
    "        similar_items = item_similarity_df.loc[item_id]\n",
    "        user_ratings = item_features.loc[:, user_id]\n",
    "        weighted_sum = np.dot(similar_items, user_ratings)\n",
    "        sum_of_weights = np.sum(similar_items)\n",
    "        if sum_of_weights > 0:\n",
    "            return weighted_sum / sum_of_weights\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Predict ratings for the test subset in batches\n",
    "batch_size = 1000  \n",
    "test_df_subset = combined_df_subset.sample(frac=0.2, random_state=42)\n",
    "\n",
    "predicted_ratings = []\n",
    "for start in range(0, len(test_df_subset), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch = test_df_subset.iloc[start:end]\n",
    "    batch_predictions = batch.apply(lambda row: predict_ratings_batch(row['user_id'], row['item_id']), axis=1)\n",
    "    predicted_ratings.extend(batch_predictions)\n",
    "\n",
    "test_df_subset['predicted_rating'] = predicted_ratings\n",
    "\n",
    "# Evaluation for content-based filtering\n",
    "cb_rmse = mean_squared_error(test_df_subset['rating'], test_df_subset['predicted_rating'], squared=False)\n",
    "cb_mae = mean_absolute_error(test_df_subset['rating'], test_df_subset['predicted_rating'])\n",
    "\n",
    "print(f'Content-Based Filtering RMSE: {cb_rmse}')\n",
    "print(f'Content-Based Filtering MAE: {cb_mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Define a function to load data in chunks\n",
    "def load_data_in_chunks(file_path, chunk_size=10000):\n",
    "    chunk_list = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        chunk_list.append(chunk)\n",
    "    return pd.concat(chunk_list, axis=0)\n",
    "\n",
    "# Load pre-processed data in chunks\n",
    "file_path = 'preprocessed_combined_data.csv'\n",
    "combined_df = load_data_in_chunks(file_path, chunk_size=10000)\n",
    "\n",
    "# Shuffle the entire dataset\n",
    "combined_df_subset = combined_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Create a pivot table for items and features\n",
    "item_features = combined_df_subset.pivot_table(index='item_id', columns='user_id', values='rating').fillna(0)\n",
    "\n",
    "# Convert the pivot table to a sparse matrix\n",
    "item_features_sparse = csr_matrix(item_features.values)\n",
    "\n",
    "# Calculate cosine similarity between items\n",
    "item_similarity = cosine_similarity(item_features_sparse)\n",
    "\n",
    "# Convert the similarity matrix to a DataFrame for easier handling\n",
    "item_similarity_df = pd.DataFrame(item_similarity, index=item_features.index, columns=item_features.index)\n",
    "\n",
    "# Function to predict ratings using item similarity\n",
    "def predict_ratings_batch(user_id, item_id):\n",
    "    if item_id in item_features.index:\n",
    "        similar_items = item_similarity_df.loc[item_id]\n",
    "        user_ratings = item_features.loc[:, user_id]\n",
    "        weighted_sum = np.dot(similar_items, user_ratings)\n",
    "        sum_of_weights = np.sum(similar_items)\n",
    "        if sum_of_weights > 0:\n",
    "            return weighted_sum / sum_of_weights\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Predict ratings for the test subset in batches\n",
    "batch_size = 1000\n",
    "test_df_subset = combined_df_subset.sample(frac=0.1, random_state=42)  # Use 10% of the full dataset for testing\n",
    "\n",
    "predicted_ratings = []\n",
    "for start in range(0, len(test_df_subset), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch = test_df_subset.iloc[start:end]\n",
    "    batch_predictions = batch.apply(lambda row: predict_ratings_batch(row['user_id'], row['item_id']), axis=1)\n",
    "    predicted_ratings.extend(batch_predictions)\n",
    "\n",
    "test_df_subset['predicted_rating'] = predicted_ratings\n",
    "\n",
    "# Evaluation for content-based filtering\n",
    "cb_rmse = mean_squared_error(test_df_subset['rating'], test_df_subset['predicted_rating'], squared=False)\n",
    "cb_mae = mean_absolute_error(test_df_subset['rating'], test_df_subset['predicted_rating'])\n",
    "\n",
    "print(f'Content-Based Filtering RMSE: {cb_rmse}')\n",
    "print(f'Content-Based Filtering MAE: {cb_mae}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
